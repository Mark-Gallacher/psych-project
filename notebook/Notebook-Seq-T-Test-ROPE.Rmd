---
title: "Sequential T-tests with Rope"
author: "Mark Gallacher"
date: "2022-11-04"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### **Chapter 0** : Setting up some Parametres and Loading in Librarys + Functions 
```{r}
library(tidyverse)
source(file = here::here("my-scripts", "R", "t_test_loop_functions.R"))

n = seq(5, 5000, length.out = 1000)
l_n = length(n)

alphas = c(0.05, 0.01, 0.005, 0.001, 0.0005)
l_a = length(alphas)

repeats = 1000
```
### **Chapter 1** : Introducing Sequential Testing and the ROPE

In Null hypothesis significant testing (NHST) the standard practice is to construct a pseudo-hypothesis which represents the absence of an effect. This is typically done by stating the mean difference between the two groups would be 0. However, this "nil hypothesis" is not immune to noise in the dataset and would allow theoretically and practically irrelevant differences to reject the null hypothesis. One procedure to counter this it the ROPE, the region of practical equivalence, where we extend the values which represent the null to include values which would likely have little to no benefit in a practical setting. In other words, where the effect observed is simply too small to be useful. 

To display how the ROPE could be useful, Kruschke (2013) used sequential t-tests to produce false positives with the NHST and Bayesian framework. Sequentially testing begins with small sample sizes but increases only if the result is not significant, else it would stop the procedure at that sample size. Significant is defined in his paper as p value of less than 0.05 and a Bayes Factor of 3 and 1/3 (to support alternative hypothesis and the null hypothesis respectively). 

!["Figure A: From Kruschke (2013), where the null hypothesis signifcance testing (NHST) is compared to Bayesian approach, when the highest density inteval (HDI) is used with a region of practical equivalence (ROPE). The graphs display the inferences made at different sample sizes. For the NHST, the binary decision boundary returns a significant or non-significant difference between the two groups whereas Bayes Factors offer three options, supporting the null hypothesis, supporting the alternative or supporting neither. Sequential testing consistently provides False positives in the NHST framework, even at large sample sizes, whilst the bayesian approach appears to become more stable as sample size increases"](../images/Kruschke-seq-t-test-graph.png)

### **Chapter 2.1**: Generated Data is read into Document

Data was generated usinf sequential tests with a (Region of Practical Equivalence) ROPE with large margins to see more the extreme effects of the ROPE. The goal is to prevent the false positives that appear to naturally occur with sequential testing by preventing smaller effect rejecting the null hypothesis, especially when the null hypothesis is true.

The first two csv files contain values for 5 alpha levels (0.05, 0.01, 0.005, 0.001 and 0.0005) at different margins (0.5 and 1 cohen's d respectively). The last csv contain values for one alpha level (0.05) at 5 different margins (0, 0.1, 0.2, 0.4 and 0.8). Again more extreme values are useful to explore the effects in a more clear and obvious pattern, although they likely shouldn't be used in scientific research.

Each procedure starts at the sample size of 5, in each group (generated from a normal distribution with a mean of 0 and sd of 1), then increases by 5 if the test returns a non-significant result (Where p value is less than alpha and the mean difference is greater than the margin). The Confidence interval was not used for simplicity but it may provide more information about the observed difference, ie if there is a good chance it is actually within the ROPE but the mean is slightly outside. This is repeated till we reach the sample size of 5000 in each group, or we get a significant result. 


```{r, message= F}
# ROPE of 0.5
seq_t_test_many_alphas_rope <- read_csv(file = here::here("./sim_data/seq_t_test_many_alphas_rope.csv"))
head(seq_t_test_many_alphas_rope)
```


```{r, message= F}
# ROPE of 1
seq_t_test_many_alphas_rope_1 <- read_csv(file = here::here("./sim_data/seq_t_test_many_alphas_rope_1.csv"))
head(seq_t_test_many_alphas_rope_1)
```


```{r, message= F}
# 5 Different ROPES
seq_t_test_many_alphas_rope_m <- read_csv(file = here::here("./sim_data/seq_t_test_many_alphas_rope_m.csv"))
head(seq_t_test_many_alphas_rope_m)
```

### **Chapter 2.2**: Tidying up Data

We are trying to see the accumulated number of tests that failed at a given sample size, as we start with 1000 tests, this number gradually decreases. This will help us produce a graph showing how many have returned significant results and how many are able to "survive" to the max sample size (5000 in each group).

The new column "prop", for proportion, represents the this gradual increase in false positves. This is what is plotted later on, so it might be easier to understand these values by looking at the graphs in the next section.

Note: n now represents the total sample, as we start with 5 in each group and finish with 5,000, n now starts at 10 and ends at 10,000 (Total Sample).

```{r}
seq_t_test_many_alphas_rope_df <- seq_t_test_many_alphas_rope |> 
  group_by(alpha) |> 
  summarise(prop = cumsum(failed), 
            .groups = "drop") |> 
  mutate(n = as.integer(rep(n*2, times = l_a)))

head(seq_t_test_many_alphas_rope_df)
```


```{r}
seq_t_test_many_alphas_rope_1_df <- seq_t_test_many_alphas_rope_1 |> 
  group_by(alpha) |> 
  summarise(prop = cumsum(failed), 
            .groups = "drop") |> 
  mutate(n = as.integer(rep(n*2, times = l_a)))

head(seq_t_test_many_alphas_rope_1_df)
```


```{r}
seq_t_test_many_alphas_rope_m_df <- seq_t_test_many_alphas_rope_m |> 
  group_by(margin) |> 
  summarise(prop = cumsum(failed), 
            .groups = "drop") |> 
  mutate(n = as.integer(rep(n*2, times = l_margins)))

head(seq_t_test_many_alphas_rope_m_df)
```
### **Chapter 3.1**: Accumulated False Positive from Sequential t-tests, at different alphas or with different ROPES

Three graph represent how different alpha levels and the ROPE can influence the rate the t-tests return fail positives. The First shows the effects of a ROPE of 0.5 Cohen's d at 5 different alpha levels. The second shows an extreme value for the margin, cohen's d of 1, to see if we can get the sequential t-test to plateau and consistently fail to reject the null. 

The last graph shows alpha of 0.05, with different margins for the ROPE. Starting at 0, to show the absense of the ROPE and then 0.1 cohen's d then doubling to get the other ROPES ie (0.2, 0.4 and 0.8).

If the lines reach 1.00, that means 100% of the 1000 simuated t-test have failed up to that point. It shouldn't be interpreted as 100% of the t-tests return significant results at that one sample size. As this is a sequential procedure, this shows the accumulated false positives, ie the proportion of the tests have returned a False Positive from that sample size to the very start. To make this more clear, if we look at the first figure, with an alpha of 0.01, at a sample size of 1000 (500 in each group), we have a value around 0.50. This means that about 50% of the t-tests have returned FP from the starting point and thus only 50% remain. 

```{r, fig.height=8, fig.width=10}
seq_t_test_many_alphas_rope_df |> 
  na.omit() |> 
  ggplot(aes(x = as.integer(n), y = prop, colour = as.factor(alpha), group = alpha))+
  geom_line(size = 1)+
  scale_x_log10("N")+
  scale_y_continuous("Proportion of Tests that Returned an FP", limits = 0:1)+
  theme_minimal()+  
  ggtitle("False Positives from Sequential t-test with Rope at 0.5 Cohen's d")+
  labs(tag = "Figure 3.1A")
```


```{r, fig.height=8, fig.width=10}
seq_t_test_many_alphas_rope_1_df |> 
  na.omit() |> 
  ggplot(aes(x = as.integer(n), y = prop, colour = as.factor(alpha), group = alpha))+
  geom_line(size = 1)+
  scale_x_log10("N")+
  scale_y_continuous("Proportion of Tests that Survived", limits = 0:1)+
  theme_minimal()+
  ggtitle("False Positives from Sequential t-test with Rope at 1 Cohen's d")+
    labs(tag = "Figure 3.1B")
```

```{r, fig.height=8, fig.width=10}
seq_t_test_many_alphas_rope_m_df |> 
  na.omit() |> 
  ggplot(aes(x = as.integer(n), y = prop, colour = as.factor(margin), group = margin))+
  geom_line(size = 1)+
  scale_x_log10("N")+
  scale_y_continuous("Proportion of Tests that Survived", limits = 0:1)+
  theme_minimal()+
  ggtitle("False Positives from Sequential t-test with alpha of 0.05, at various ROPE margins")+
  labs(tag = "Figure 3.1C")
```

### **Chapter 3.2**: Showing the Mean Difference Behind the Significant Results

What may not be apparent from the False positive is the observed difference between to two groups and how the ROPE interacts with that. These graphs are quite nice illustrations of the intuition behind the ROPE, as mean differences smaller than the margins of the ROPES will not cause the sequential t-tests to stop. Even if the p-value is significant, if the mean different is smaller than the rope, the t-tests will continue on the sequential procedure. 

These graphs show how the ROPE determines to minimal accepted mean difference that would be interpreted as a significant result, as both the p-value and mean difference satify our criteria of a meaningful difference between the groups (p-value representing the rarity of the difference and the ROPE ensuring our mean differences are not irrelevant). 

Similarily as above, the first two are at 5 different alpha levels with two different ROPEs (0.5 and 1.0 cohen's d). The last graph is at one alpha level (0.05) with different ROPEs (0, 0.1, 0.2, 0.4, 0.8). 

```{r, fig.height= 8, fig.width=10}
seq_t_test_many_alphas_rope |> 
  na.omit() |> 
  ggplot(aes(x = as.integer(n), y = mean_diff, colour = as.factor(alpha), group = alpha))+
  geom_point(size = 1)+
  geom_smooth(size = 1, se = F)+
  scale_x_log10("N")+
  scale_y_continuous("Mean Difference", limits = c(0,2))+
  theme_minimal()+
  ggtitle("Mean Difference from Sequential t-test with Rope at 0.5 Cohen's d")+
  labs(tag = "Figure 3.2A")
```


```{r, fig.height= 8, fig.width=10}
seq_t_test_many_alphas_rope_1 |> 
  na.omit() |> 
  ggplot(aes(x = as.integer(n), y = mean_diff, colour = as.factor(alpha), group = alpha))+
  geom_point(size = 1)+
  geom_smooth(size = 1, se = F)+
  scale_x_log10("N")+
  scale_y_continuous("Mean Difference", limits = c(0,2))+
  theme_minimal()+
  ggtitle("Mean Difference from Sequential t-test with Rope at 1 Cohen's d")+
  labs(tag = "Figure 3.2B")
```



```{r, fig.height=8, fig.width=10}
seq_t_test_many_alphas_rope_m |> 
  na.omit() |> 
  ggplot(aes(x = as.integer(n), y = mean_diff, colour = as.factor(margin), group = margin))+
  geom_point(size = 1)+
  geom_smooth(size = 1, se = F)+
  scale_x_log10("N")+
  scale_y_continuous("Mean Difference", limits = c(0,2))+
  theme_minimal()+
  ggtitle("Mean Difference from Sequential t-test with Alpha at 0.05, at various ROPE margins")+  
  labs(tag = "Figure 3.2C")
```

### **Chapter 4.1**: Comparing Graphs when there is no ROPE - reading in data

It might be more helpful to compare these graphs to similar graphs when the ROPE is absent. Here two csv files are loaded in, the first is the proportion of FP from simulations using different alpha levels, and the second contains p values and mean differences from the same procedure. 

```{r}
seq_t_test_many_alphas <- read_csv(file = here::here("sim_data","seq_t_test_many_alphas.csv"), col_types = "ifi?")

head(seq_t_test_many_alphas)
```

Note: "count" represents how many tests passed at a given point, then prop gives the proportion. Here we seem to have been lucky as all the tests passed the very first sample size of 5 in each group, where variability in the sample is likely to be greatest. If we reran the simulations, I would expect for at least some to fail at the first hurdle. 

```{r}
seq_t_test_many_alphas_mean <- 
  read_csv(file = here::here("sim_data","seq_t_test_many_alphas_mean_break.csv"), show_col_types = F) 
  # select(1, 2, 5:8) # the other columns are not necessary here ??
  
head(seq_t_test_many_alphas_mean)
```

### **Chapter 4.2**: Accumulated False Positive from Sequential t-tests, at different alphas with no ROPE

```{r, fig.height= 8, fig.width= 10}
seq_t_test_many_alphas |> 
  ggplot(aes(x = n, y = pval, colour = alpha))+
  geom_line(aes(x = n, y = 1 - prop), size = 1, alpha = 0.7)+
  scale_x_log10("N")+
  scale_y_continuous("Proportion of Tests that Survived")+
  ggtitle("False Positives from Sequential t-test at Various Alpha Levels, with no ROPE")+
  theme_minimal()
```

### **Chapter 4.3**: Showing the Mean Difference Behind Significant Results, at different alphas with no ROPE

```{r,fig.height= 8, fig.width= 13}
seq_t_test_many_alphas_mean |>
  filter(alpha == 0.05) |> 
  ggplot(aes(x = n, y = mean_diff))+
  geom_point(size = 1, alpha = .2, colour = "blue")+
  geom_ribbon(aes(ymin = mean_diff_mean - mean_diff_sd, 
                  ymax = mean_diff_mean + mean_diff_sd), 
              alpha = .4, fill = "lightblue")+
  geom_line(aes(x = n, y = mean_diff_mean), 
            stat = "smooth", 
            size = 1, colour = "darkblue", alpha = .6)+
  theme_minimal()+
  scale_x_log10("Sample Size")+
  scale_y_continuous("Absolute Mean Difference")+
  ggtitle("Mean Difference of Significant results at different sample sizes, Alpha = 0.05")+
  coord_cartesian(expand = T)
```

```{r, fig.height= 8, fig.width= 13}
seq_t_test_many_alphas_mean |> 
  ggplot(aes(x = n, y = mean_diff, colour = as.factor(alpha)))+
  geom_point(size = 1, alpha = .2)+
  geom_line(aes(y = mean_diff_mean),stat = "smooth",  size = 1, alpha = .4, se = F)+
  theme_minimal()+
  scale_x_log10("Sample Size")+
  scale_y_continuous("Absolute Mean Difference")+
  ggtitle("Mean Difference of Significant results at different sample sizes (for different alphas)")
```
